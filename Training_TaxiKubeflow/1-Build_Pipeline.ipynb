{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "One very popular data science example is the Taxi Cab (or Chicago Taxi) example that predicts trips that result in tips greater than 20% of the fare. We are going to showcase the same example on Kubeflow running on HPECP 5.2. We have updated few Docker images used in the example to store the UI metadata and Metrics artifacts in internal MinIO used by Kubeflow. Weâ€™ll explain all the changes in each of pipeline steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and setting up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from kfp import gcp\n",
    "from kfp import onprem\n",
    "from kubernetes.client.models import V1EnvVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up connection to Kubeflow's minio to store our pipeline visualizations \n",
    "secretKey = V1EnvVar(name='MINIO_SECRET_KEY', value='minio123')\n",
    "accessKey = V1EnvVar(name='MINIO_ACCESS_KEY', value='minio')\n",
    "minio_endpoint = V1EnvVar(name='MINIO_ENDPOINT', value='minio-service:9000')\n",
    "\n",
    "platform = 'local'\n",
    "proxy = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading yaml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['http_proxy'] = \"http://web-proxy.corp.hpecorp.net:8080\"\n",
    "# os.environ['https_proxy'] = \"http://web-proxy.corp.hpecorp.net:8080\"\n",
    "\n",
    "dataflow_tf_data_validation_op  = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tfdv_component.yaml')\n",
    "dataflow_tf_transform_op        = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tft_component.yaml')\n",
    "tf_train_op                     = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/dnntrainer_component.yaml')\n",
    "dataflow_tf_model_analyze_op    = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/tfma_component.yaml')\n",
    "dataflow_tf_predict_op          = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/predict_component.yaml')\n",
    "\n",
    "confusion_matrix_op             = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/confusion_matrix_component.yaml')\n",
    "roc_op                          = components.load_component_from_url('https://raw.githubusercontent.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio/main/yamls/roc_component.yaml')\n",
    "\n",
    "# os.environ['http_proxy'] = \"\"\n",
    "# os.environ['https_proxy'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating kubeflow pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def taxi_cab_classification(\n",
    "    project,\n",
    "    output=\"/mnt/shared\",\n",
    "    column_names='/mnt/shared/pipelines/column-names.json',\n",
    "    key_columns='trip_start_timestamp',\n",
    "    train='/mnt/shared/pipelines/train.csv',\n",
    "    evaluation='/mnt/shared/pipelines/eval.csv',\n",
    "    mode='local',\n",
    "    preprocess_module='/mnt/shared/pipelines/preprocessing.py',\n",
    "    learning_rate=0.1,\n",
    "    hidden_layer_size='1500',\n",
    "    steps=3000,\n",
    "    analyze_slice_column='trip_start_hour'\n",
    "):\n",
    "    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\n",
    "    target_lambda = \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\"\n",
    "    target_class_lambda = \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\"\n",
    "\n",
    "    tf_server_name = 'taxi-cab-classification-model-{{workflow.uid}}'\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Creating PVC and cloning GIT repo \n",
    "    '''\n",
    "    if platform != 'GCP':\n",
    "        vop = dsl.VolumeOp(\n",
    "            name=\"create_pvc\",\n",
    "            resource_name=\"pipeline-pvc\",\n",
    "            modes=dsl.VOLUME_MODE_RWM,\n",
    "            size=\"1Gi\"\n",
    "        )\n",
    "    if proxy != \"\":\n",
    "        checkout = dsl.ContainerOp(\n",
    "        name=\"checkout\",\n",
    "        image=\"alpine/git:latest\",\n",
    "        command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio.git\", str(output) + \"/pipelines\", \"-c\", \"http.proxy={}\".format(proxy)],\n",
    "    ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n",
    "    else:\n",
    "        checkout = dsl.ContainerOp(\n",
    "        name=\"checkout\",\n",
    "        image=\"alpine/git:latest\",\n",
    "        command=[\"git\", \"clone\", \"https://github.com/II-VSB-II/TaxiClassificationKubeflowPipelineMinio.git\", str(output) + \"/pipelines\"],\n",
    "    ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n",
    "\n",
    "\n",
    "    checkout.after(vop)\n",
    "\n",
    "    '''\n",
    "    TFDV can compute descriptive statistics that provide a quick overview of the data \n",
    "    in terms of the features that are present and the shapes of their value distributions\n",
    "    \n",
    "    Here we are going to validate our dataset and infer our schema \n",
    "    '''\n",
    "    validation = dataflow_tf_data_validation_op(\n",
    "        inference_data=train,\n",
    "        validation_data=evaluation,\n",
    "        column_names=column_names,\n",
    "        key_columns=key_columns,\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        validation_output=output_template,\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    using tf.Transform to do our data preprocessing and feature transformations\n",
    "    here we are generating a boolean feature if a tip was > 20% and saving the features\n",
    "    '''\n",
    "    preprocess = dataflow_tf_transform_op(\n",
    "        training_data_file_pattern=train,\n",
    "        evaluation_data_file_pattern=evaluation,\n",
    "        schema=validation.outputs['schema'],\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        preprocessing_module=preprocess_module,\n",
    "        transformed_data_dir=output_template\n",
    "    )\n",
    "    \n",
    "    '''\n",
    "    train a neural network model with Tensorflow\n",
    "    '''\n",
    "    training = tf_train_op(\n",
    "        transformed_data_dir=preprocess.output,\n",
    "        schema=validation.outputs['schema'],\n",
    "        learning_rate=learning_rate,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        steps=steps,\n",
    "        target='tips',\n",
    "        preprocessing_module=preprocess_module,\n",
    "        training_output_dir=output_template\n",
    "    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\n",
    "\n",
    "    '''\n",
    "    analyze model and save visualizations into minio\n",
    "    '''\n",
    "    analysis = dataflow_tf_model_analyze_op(\n",
    "        model=training.output,\n",
    "        evaluation_data=evaluation,\n",
    "        schema=validation.outputs['schema'],\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        slice_columns=analyze_slice_column,\n",
    "        analysis_results_dir=output_template\n",
    "    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    generate predictions into a table with it's features and save visualization into minio\n",
    "    '''\n",
    "    prediction = dataflow_tf_predict_op(\n",
    "        data_file_pattern=evaluation,\n",
    "        schema=validation.outputs['schema'],\n",
    "        target_column='tips',\n",
    "        model=training.output,\n",
    "        run_mode=mode,\n",
    "        gcp_project=project,\n",
    "        predictions_dir=output_template\n",
    "    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\n",
    "\n",
    "\n",
    "    '''\n",
    "    generate confusion matrix and save into minio\n",
    "    '''\n",
    "    cm = confusion_matrix_op(\n",
    "        predictions=prediction.output,\n",
    "        target_lambda=target_lambda,\n",
    "        output_dir=output_template\n",
    "    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\n",
    "\n",
    "    \n",
    "    ''' \n",
    "    generate ROC curve and save into minio\n",
    "    '''\n",
    "    roc = roc_op(\n",
    "        predictions_dir=prediction.output,\n",
    "        target_lambda=target_class_lambda,\n",
    "        output_dir=output_template\n",
    "    ).add_env_variable(secretKey).add_env_variable(accessKey).add_env_variable(minio_endpoint)\n",
    "\n",
    "\n",
    "    steps = [validation, preprocess, training, analysis, prediction, cm, roc]\n",
    "    \n",
    "    '''\n",
    "    mount the pvc to each of the pipeline steps \n",
    "    '''\n",
    "    for step in steps:\n",
    "        step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    \"project\" : \"project\",\n",
    "    \"output\" : \"/mnt/shared\",\n",
    "    \"column_names\" : '/mnt/shared/pipelines/column-names.json',\n",
    "    \"key_columns\" :'trip_start_timestamp',\n",
    "    \"train\" : '/mnt/shared/pipelines/train.csv',\n",
    "    \"evaluation\": '/mnt/shared/pipelines/eval.csv',\n",
    "    \"mode\" : 'local',\n",
    "    \"preprocess_module\": '/mnt/shared/pipelines/preprocessing.py',\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"hidden_layer_size\": '1500',\n",
    "    \"steps\": 3000,\n",
    "    \"analyze_slice_column\": 'trip_start_hour'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiles pipeline into a zip file in which you can upload directly to the KFP UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(taxi_cab_classification, \"TaxiPipelineMinio\" + '.zip', arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing pipeline directly from KubeDirector notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import json\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "\n",
    "# #For a non ssl system, enter the kubeflow dashboard url with '/pipeline' at the end.\n",
    "# url = ''\n",
    "\n",
    "# from ezmeral_kf_utils import KfSession\n",
    "# K = KfSession(url)\n",
    "# client=K.kf_client()\n",
    "\n",
    "# #For as ssl enabled system, set both kubeflow url with '/pipeline' at the end and location of certificate.\n",
    "# url = ''\n",
    "# cert = ''\n",
    "\n",
    "# from ezmeral_kf_utils import KfSession\n",
    "# K = KfSession(url,cert)\n",
    "# client=K.kf_client()\n",
    "\n",
    "\n",
    "# client.create_run_from_pipeline_func(\n",
    "#     taxi_cab_classification, \n",
    "#     experiment_name=\"Taxi Cab\",\n",
    "#     arguments=arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Executing directly from Kubeflow notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/ba1f8582-37b6-493f-9992-ff13fd9e5bba\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/9294c3d9-aaa4-4be7-8109-6b1892f63b4c\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=9294c3d9-aaa4-4be7-8109-6b1892f63b4c)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.Client().create_run_from_pipeline_func(\n",
    "    taxi_cab_classification, \n",
    "    experiment_name=\"Taxi Cab\",\n",
    "    arguments=arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now look at the Kubeflow pipelines UI and watch the training happen! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "devsds/kubeflow-kale@sha256:2783735efc637b872fe3f9ef7dae9358d0655c30770d618ab2eb61ba088ef032",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
