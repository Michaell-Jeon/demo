{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Modeling \n",
    "1. Define a model \n",
    "2. Experiment and hyper parameter tuning\n",
    "3. Train model on training cluster\n",
    "4. Save model back into project repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Loading student variable\n",
    "%store -r STUDENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProjectRepo(path):\n",
    "   ProjectRepo = \"/bd-fs-mnt/project_repo\"\n",
    "   return str(ProjectRepo + '/' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = pd.read_csv(ProjectRepo('data/' + STUDENT + '_UCI_Income/adult_train_cleaned.csv'))\n",
    "final_test = pd.read_csv(ProjectRepo('data/' + STUDENT + '_UCI_Income/adult_test_cleaned.csv'))\n",
    "y_train = final_train.pop('wage_class')\n",
    "y_test = final_test.pop('wage_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {'max_depth': [3,5], 'min_child_weight': [3,5]}\n",
    "ind_params = { 'n_estimators': 100, 'seed': 0, 'subsample' : 0.8, 'colsample_bytree': 0.8, \n",
    "              'objective': 'binary:logistic', \"eval_metric\" :\"error\"}\n",
    "\n",
    "#optimizing for accuracy, GBM = gradient boost model\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params, use_label_encoder=False), \n",
    "                             cv_params, \n",
    "                             scoring = 'accuracy', cv = 2, n_jobs = 1, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training of this model could take a few minutes or more depending on the infrastructure you are running on. Please be patient \n",
    "optimized_GBM.fit(final_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_GBM.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model\n",
    "Tuning other hyperparameters in an attempt to achieve higher mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {'learning_rate': [0.1, 0.01], 'subsample': [0.7, 0.8, 0.9]}\n",
    "ind_params = {'n_estimators': 100, 'seed': 0, 'colsample_bytree': 0.8, 'objective': 'binary:logistic', \n",
    "              'max_depth': 3, 'min_child_weight': 1, \"eval_metric\" :\"error\"}\n",
    "                    \n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params, use_label_encoder=False), \n",
    "                             cv_params, \n",
    "                             scoring = 'accuracy', cv=2, n_jobs=1, verbose=3)\n",
    "optimized_GBM.fit(final_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_GBM.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third model\n",
    "Utilize XGBoost's built-in cv which allows early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgdmat = xgb.DMatrix(final_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_params = {'eta': 0.1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'objective': 'binary:logistic',\n",
    "              'max_depth': 3, 'min_child_weight': 1}\n",
    "\n",
    "cv_xgb = xgb.cv(params=our_params, dtrain=xgdmat, num_boost_round=3000, metrics=['error'],\n",
    "                early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best iteration:', len(cv_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_params = {'eta': 0.1, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth':3, 'min_child_weight':1} \n",
    "\n",
    "final_gb = xgb.train(our_params, xgdmat, num_boost_round = 326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(final_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = final_gb.get_fscore()\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_frame = pd.DataFrame({'Importance': list(importances.values()), 'Feature': list(importances.keys())})\n",
    "importance_frame.sort_values(by = 'Importance', inplace=True)\n",
    "importance_frame.plot(kind='barh', x='Feature', figsize=(8,8), color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on the remote shared training cluster\n",
    "\n",
    "In general, data scientists use their local Jupyter Notebook to **experiment** several learning algorithms with a variety of parameters. They do so to determine the ML model that works best for the business problem they try to address and develop the model that yields to the best prediction result. Then, within their notebooks, they submit their code to large scaled computing training cluster environment to train and test their full ML models, in a reasonable time, typically against a larger training dataset and test dataset. The output of this step is a trained model ready for deployment in production.\n",
    "\n",
    ">**Note:** _This workshop is not intended to teach you about AI/ML model experimentation and development. It is intended to give a use case for data science end-to-end ML workflow with HPE Ezmeral ML Ops. Therefore we will assume that the experimentation step has already been done and that the data science team has shared the best performant ML model in a notebook in the GitHub version control system repository set up by the Operations team for the data science team. The notebook is actually this notebook pulled from GitHub repository by the local Jupyter Notebook cluster. Here you will submit the ML model code to the tenant-shared training cluster environment to train and test your model against the train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%attachments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in your initials for the <b>STUDENT</b> variable and your training cluster name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%<your training cluster>\n",
    "\n",
    "STUDENT = \"\"\n",
    "\n",
    "# Importing libraries \n",
    "print(\"Importing libraries\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Start time \n",
    "print(\"Start time: \", datetime.datetime.now())\n",
    "\n",
    "# Project repo path function\n",
    "def saveInProjectRepo(path):\n",
    "   ProjectRepo = \"/bd-fs-mnt/project_repo/\"\n",
    "   return str(ProjectRepo + '/' + path)\n",
    "\n",
    "# Reading in data \n",
    "print(\"Reading in data\")\n",
    "train = pd.read_csv(saveInProjectRepo('data/' + STUDENT + '_UCI_Income/adult_train_cleaned.csv'))\n",
    "print(\"Done reading in data\")\n",
    "\n",
    "# Extracting target values \n",
    "y_train = train.pop('wage_class')\n",
    "train.pop('Unnamed: 0')\n",
    "\n",
    "# Model development / Training\n",
    "print(\"Training...\")\n",
    "xgdmat = xgb.DMatrix(train, y_train)\n",
    "our_params = {'eta': 0.1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'objective': 'binary:logistic',\n",
    "              'max_depth': 3, 'min_child_weight': 1, \"eval_metric\" :\"error\"}\n",
    "cv_xgb = xgb.cv(params=our_params, dtrain=xgdmat, num_boost_round=3000, metrics=['error'],\n",
    "                early_stopping_rounds=100)\n",
    "optimal_rounds = len(cv_xgb)\n",
    "final_gb = xgb.train(our_params, xgdmat, num_boost_round = optimal_rounds)\n",
    "\n",
    "# Save model into project repo\n",
    "print(\"Saving model\")\n",
    "# pickle.dump(final_gb, open( saveInProjectRepo('models/XGB_Income/') + \"XGB.pickle.dat\", \"wb\"))\n",
    "xgb.Booster.save_model(final_gb, saveInProjectRepo('models/' + STUDENT + '_UCI_Income/') + \"XGB.pickle.dat\")\n",
    "\n",
    "# Finish time\n",
    "print(\"End time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the history url from the output of the previous cell\n",
    "%logs --url http://training-loadbalancer-dcdpw-0.training76xcc.terry-mlops.svc.cluster.local:10001/history/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with loading pickle model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = pd.read_csv(ProjectRepo('data/' + STUDENT + '_UCI_Income/adult_test_cleaned.csv'))\n",
    "cleaned.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scoring with pickle model yields proper results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running with final_gb model from local notebook \n",
    "temp = cleaned.tail(1)\n",
    "y_test = temp.pop('wage_class')\n",
    "temp.set_index('age')\n",
    "# temp.pop('Unnamed: 0')\n",
    "mat = xgb.DMatrix(temp) \n",
    "y_pred = final_gb.predict(mat)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.Booster({'nthread':325})\n",
    "model.load_model(ProjectRepo('models/' + STUDENT + '_UCI_Income/XGB.pickle.dat'))\n",
    "temp = cleaned.tail(1)\n",
    "y_test = temp.pop('wage_class')\n",
    "temp.set_index('age')\n",
    "temp.pop('Unnamed: 0')\n",
    "mat = xgb.DMatrix(temp) \n",
    "y_pred = model.predict(mat)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue onto Lab 4 for model serving! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
